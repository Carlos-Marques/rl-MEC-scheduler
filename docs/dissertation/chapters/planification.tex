\chapter{Conclusions}
\section{Summary}
\noindent As presented in Chapter \ref{chap:stateoftheart} Section \ref{section:MECarch} several architectures have been proposed in the \acrshort{MEC} context that present complex optimization problems of managing how to distribute tasks between \acrshort{UE}s and \acrshort{MEC} servers. Due to the high dimensional complexity and uncertain networking conditions, classical offline optimization algorithms fail to effectively manage these types of problems. As a solution to this, a review of the state of the art in \acrshort{DRL} and its application to these \acrshort{MEC} challenges was made in Chapter \ref{chap:stateoftheart} Sections \ref{section:RL}, \ref{section:EE} and \ref{section:RW}. Based on the work done by papers \cite{taskclass1} and \cite{NUE1mec} a more complete system that takes into account the possibility of offloading between a network of heterogeneous \acrshort{UE}s to a network of heterogeneous \acrshort{MEC} servers is proposed. As far as the candidate knows, this is the first time this extended optimization problem is addressed. In order to deal with the increased complexity of taking into account computation, battery, delay and communication constraints in an \emph{N} to \emph{N} problem, a network manager agent is proposed in Section \ref{solution}. In order to evaluate the performance of the proposed agent, several baselines are described in Section \ref{baselines}. Finally, the agent's capabilities are put to the test in Sections \ref{simple_test}, \ref{scalability_data}, \ref{robustness_stability} and \ref{reward_section}.

In Section \ref{simple_test}, a simple test case is used to demonstrate the simulator, baselines and the agent's capacity to learn. As expected, the Full Local baseline performs the worst, then the Random Offload baseline and finally the Full nearest MEC. The proposed agent is shown to learn, overperforming the baselines by 96.65\%, 82.89\% and 24.06\%, respectively. The agent achieves this while not requiring any information of the network topology by simply interacting with the environment.

In Section \ref{scalability_data}, the agent's scalability and data efficiency is put to the test by increasing the complexity of the system with two new tests. Given that the system complexity is tightly related with the number of \acrshort{UE}s in the network, the agent was tested in an environment with 10 and 20 \acrshort{UE}s. As showcased, the agent learned to overperform the best baseline with a 22.17\% and 34.38\% improvement, respectively, showcasing its ability to scale with problem complexity. The agent also achieved this in 55K steps, demonstrating it's data efficiency.

Section \ref{robustness_stability} serves to test the agent's robustness and stability. In order to test robustness, two new test cases were implemented. First, the agent's robustness to changing network conditions was tested and proven by overperforming the baseline with a 30.62\% improvement. Secondly, the agent's robustness to an environment with heterogeneous computation capabilities of \acrshort{UE}s and \acrshort{MEC} servers was put to the test. The agent not only was able to learn and overperform the best baseline, but it did so in a greater faction than with simpler tests, demonstrating a reduction of 30.39\% over the Full nearest MEC baseline, compared with the 22.17\% of the same test without heterogeneous computation capacity. Stability was analysed by looking at the different learning curves of the agent. Over all previous tests, the agent was shown to be stable independently of the changing conditions, never showing regression or collapse of its reward after it starts learning the environment.

In the end, Section \ref{reward_section} presents the effects of changing the importance weight distribution from the overall performance to the worst-case performance. The adjustability of the reward function is shown to allow the improvement of the worst-case cost at the expense of the overall system cost.

All the simulator, baselines and agent code, as well as environment configurations, can be found in the following code repository:
\href{https://github.com/Carlos-Marques/rl-MEC-scheduler}{https://github.com/Carlos-Marques/rl-MEC-scheduler}.

\section{Future Work}
\noindent Future steps of research should include:
\begin{itemize}
    \item Expanding the proposed simulator to include more dynamic environments, implementing the ability to change the following as simulation progresses:
    \begin{itemize}
      \item \acrshort{UE} locations to simulate mobility;
      \item The number of \acrshort{UE}s and \acrshort{MEC} servers;
      \item Network conditions.
    \end{itemize}
    \item Creating a more complete test set, testing environments for specific scenarios. For example, a Smart city environment or an autonomous vehicle environment. 
    \item Exploring the use of this orchestration agent with choreography algorithms like the one proposed in \cite{Lulu}, by simulating an environment where several groups of \acrshort{MEC} servers would be orchestrated by agents like the one proposed in this work, while choreographing with each other.
    \item Exploring the use of model based \acrshort{RL} algorithms to take advantage of the extensive domain knowledge of telecommunication physics. By leveraging this knowledge, the agent could converge to a better solution more quickly without having to learn this through experimentation.
\end{itemize}